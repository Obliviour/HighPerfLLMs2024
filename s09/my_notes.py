# High performance inference:

# Pass a single token in, not multiple  or use kv cache

# Process 30 tokens... aka more tokens

# Interesting debug experience is printing out dimensions while building the model.
